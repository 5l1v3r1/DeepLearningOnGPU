深度学习现状
============


深度学习源于机器学习理论。1980年代末期，用于人工神经网络的反向传播算法（Back Propagation，BP）的发明\cite{fahlman1988faster,lecun1989backpropagation}，给机器学习带来了希望，由此掀起了基于统计模型的机器学习热潮。该算法可从大量的训练样本中学习出统计规律，从而对未知事件做出预测，这种结构只有一层隐层节点，因此被称为浅层学习模型。但是BP算法基于局部梯度下降，从一些随机初始点开始训练，通常陷入局部极值，并随着网络深度的增加而持续恶化，不能很好地求解深度神经网络结构问题。20世纪90 年代，各种浅层机器学习模型相继被提出，比如支撑向量机（Support Vector Machines，SVM）\cite{burges1998tutorial}、Boosting\cite{berger1996maximum}、最大熵方法\cite{berger1996maximum}， Logistic Regression\cite{peduzzi1996simulation}等。这些模型也只有一层隐层节点（比如 Boosting、SVM)，或没有隐层节点的结构（比如LR）。这些模型在无论是理论分析，还是实际应用都获得了巨大的成功。图像是深度学习最早尝试的应用领域。早在1989年，Yann LeCun 的团队就发表了卷积神经网络（Convolution Neural Networks, CNN）的工作\cite{lecun1998gradient}。CNN是一种带有卷积结构的深度神经网络，通常至少有两个非线性可训练的卷积层，两个非线性的固定卷积层（又叫Pooling Layer）和一个全连接层，一共至少5个隐含层。CNN的结构受到著名的Hubel-Wiesel生物视觉模型的启发，尤其是模拟视觉皮层V1和V2层中Simple Cell和Complex Cell的行为。在很长时间里，CNN 虽然在小规模的问题上，如手写数字，取得过当时世界最好结果，但一直没有取得巨大成功。这主要原因是，CNN 在大规模图像上效果不好，比如像素很多的自然图片内容理解，所以没有得到计算机视觉领域的足够重视。这个情况一直持续到2012年10月，Geoffrey Hinton和他的两个学生在著名的ImageNet 问题上用更深的CNN 取得世界最好结果，使得图像识别大踏步前进。在Hinton的模型里，输入就是图像的像素，没有用到任何的人工特征。但是由于理论分析的难度，训练方法需要很多经验和技巧，以及对数据和计算的要求，这时浅层人工神经网络处于较为沉默的状态。

2006年，加拿大多伦多大学教授 Hinton的团队在《科学》、Neural computation 和 NIPS上发表了4 篇文章\cite{hinton2006fast,hinton2006reducing,hinton2007learning,bengio2007greedy}，提出基于深度信任网络(deep belief network，DBN)的无监督学习算法，解决了深度学习模型优化中局部解的问题。这些文章有两个主要的信息：1. 隐层的人工神经网络具有优异的特征学习能力，学习到的特征对数据有更本质的刻画，从而有利于对数据进行可视化或分类；2.可以通过“逐层初始化”（Layer-wise Pre-training）来有效克服深度神经网络在训练上的难度，在与网络大小和深度呈线性的时间复杂度上优化DBN的权值，将求解的问题分解成为若干更简单的子问题进行求解。这个模型包含一个可视层，多个隐含层，因此也被成为深度学习（Deep Learning, DL）模型。

深度学习比浅学习具有更强的表示能力，而由于深度的增加使得非凸目标函数产生的局部最优解是造成学习困难的主要因素。从具有开创性的文献发 表之后，大量研究人员对深度学习进行了广泛的研究以提高和应用深度学习技术。Bengio和Ranzato 等人提出用无监督学习初始化每一层神经网络\cite{poultney2006efficient}； Ethan 等人尝试理解无监督学习对深度学习过程起帮助作用的原因\cite{erhan_why_2010}；Glorot等人 研究深度结构神经网络的原始训练过程失败的原因\cite{glorot2010understanding}。 文献\cite{bengio_learning_2009} 对深度学习进行了较为全面的综述，基于无监督学习技术提出贪婪逐层预训练学习过程用于初始化深度学习模 型的参数，从底层开始训练每层神经网络形成输入的表示，在 无监督初始化之后，堆栈各层神经网络转换为深度监督前馈神 经网络，用梯度下降进行微调。用于深度学习的学习方法主要 集中在学习数据的有用表示，在神经网络较高层中使学习到的 特征不随变化的因素而变化，对实际数据中的突发变化具有更 强的鲁棒性\cite{goodfellow2009measuring}。 文献\cite{hinton2010practical}给出了训练深度学习模型的相关技巧，尤其是受限玻尔兹曼机(restricted Boltzmann machine， RBM)\cite{hinton2006reducing,hinton2010practical,larochelle2008classification}，许多来自神经网络训练的想法也可以用于深度结构神 经网络学习。Bengio 在文献\cite{bengio2012practical} 中给出了用于不同种类深度结构的神经网络的训练方法。已经
在语音识别\cite{jia2014caffe,huang2014deep}、图像识别\cite{ouyang_multi-source_2014,zeng_multi-stage_2013,sermanet_pedestrian_2013,luo_pedestrian_2013,luo_switchable_2014}、 自然语言处理\cite{sarikaya2014application,manning2014stanford}，用户行为分析\cite{o2014potential,hu2014discriminative,shao2014spatio}、 在线广告推送等领域取得显著进展。

自2006年以来，深度学习在整个学术界持续升温。斯坦福大学、纽约大学、加拿大蒙特利尔大学等成为研究深度学习的重镇。2010年，美国国防部DARPA 计划首次资助深度学习项目，参与方有斯坦福大学、纽约大学和NEC美国研究院。支持深度学习的一个重要依据，就是脑神经系统的确具有丰富的层次结构。一个最著名的例子就是Hubel-Wiesel 模型，由于揭示了视觉神经的机理而曾获得诺贝尔医学与生理学奖。除了仿生学的角度，目前深度学习的理论研究还基本处于起步阶段，但在应用领域已显现出巨大的能量。

2012年6月，《纽约时报》披露了Google Brain项目\cite{markoff2012many}，引起了公众的广泛关注。这个项目是由著名的斯坦福大学机器学习教授Andrew Ng和在大规模计算机系统方面的世界顶尖专家Jeff Dean共同主导，用16000个CPU Core的并行计算平台训练一种称为“深层神经网络”（DNN，Deep Neural Networks）的机器学习模型，在语音识别和图像识别等领域获得了巨大的成功。2012年11月，微软在中国天津的一次活动上公开演示了一个全自动的同声传译系统，讲演者用英文演讲，后台的计算机一气呵成自动完成语音识别、英中机器翻译，以及中文语音合成，效果非常流畅。据报道，后面支撑的关键技术也是深度学习算法\cite{markoff2012scientists}。2013年1月，在百度的年会上，创始人兼CEO 李彦宏高调宣布要成立百度研究院，其中第一个重点方向就是深度学习，并为此而成立Institute of Deep Learning（IDL）。2013年4月，《麻省理工学院技术评论》杂志将深度学习列为2013 年十大突破性技术（Breakthrough Technology）之首。

相比早期的浅层次学习，深度学习具有许多优点：

a)在网络表达复杂目标函数的能力方面，浅结构神经网络有时无法很好地实现高变函数等复杂高维函数的表示，而用深度结构神经网络能够较好地表征。

b)在网络结构的计算复杂度方面，当用深度为k的网络结构能够紧凑地表达某一函数时，在采用深度小于k的网络结构表达该函数时，可能需要增加指数级规模数量的计算因子，大大增加了计算的复杂度。另外，需要利用训练样本对计算因子中的参数值进行调整，当一个网络结构的训练样本数量有限而计算因子数量增加时，其泛化能力会变得很差。

c)在仿生学角度方面，深度学习网络结构是对人类大脑皮层的最好模拟。与大脑皮层一样，深度学习对输入数据的处理是分层进行的，用每一层神经网络提取原始数据不同水平的特征。

d)在信息共享方面，深度学习获得的多重水平的提取特征可以在类似的不同任务中重复使用，相当于对任务求解提供了一些无监督的数据，可以获得更多的有用信息。

早期的深度学习都是基于单个特征的学习，这样难以模拟网路中复杂的关系，从而难以获得很好的网络预测。因此我们提出基于深度学习的网络预测，学习网络的深层次结构。


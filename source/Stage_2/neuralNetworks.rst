神经网络笔记
************

神经网络框架
=============

.. graphviz::
   
   digraph{
      a [label="初始层"]
      b [label="隐层1（中层特征：corner,edge and color？）"]
      c [label="隐层2（高层特征：shape？）"]
      d [label="决策层"]
      a->b  [label="卷积层"]
      c->b  [label="auto-coder"]
      b->d  [label="跨层网络实现多尺度特征"] 
      b->c  [label="使用sparse coding层层抽象"]
      c->d  [label="全局特征决策"]
   }

工具
=====

#. 神经网络设计工具：（CAFFE，theao）。
#. 可视化：信息流的可视化（编码），训练时的可视化(过拟合，迭代下降)。
#. 网络拓扑本身的可视化。

feedback
=========

#. 从整个学习上说，就是一个偏导函数量。要解决的一个问题包括两方面：第一个是学习速度问题，第二个是防止震荡。目前用的都是基于导数的优化。
#. 受到cost函数影响是约束问题的松和紧。 cost ， activate MSE(最小二乘），线性的max 函数 cross-entropy, sigmoid函数 log-likelihood, softmax函数。
   目标评价函数，就是测量函数，也可以当做一个相似度的计算，或者距离远近的计算。至于根据这个距离，决定下一步如何行动，那就是loss 函数需要来决定的。 常用的最小二乘法，不同的cost函数与梯度计算中可以计算出不同的W,b. 如果
#. 超参数的估计，目前是过拟合产生一个主要原因。
#. 具体采用哪一种组合呢，就看你采用哪一种解析了，如果想用要概率模型就要用softmax组合。

神经元结构：
============

#. 神经元从结构功能上来说：包括感知神经元和激活神经元。
#. 从感知神经元包括：线性的，卷积，限制玻尔兹曼机。
#. 激活函数包括（只要是连续即可，并且有界，导数具有正交性质）：sigmod，tanh，softmax，retified linear 。
   sigmod与tanh是可以相互转化的 sigmod=1/2(1+tanh(z/2))，不同的激活函数会不同的符号性质与函数导数性质，是会带来不同的效果。其证明可见下边。现在还有足够的证据证明，哪一种activate函数是最好的。是根据应用需要来确定的吧。 
   http://neuralnetworksanddeeplearning.com/chap3.html 公式111
   http://www.wolframalpha.com/input/?i=tanh%28x%29
是利用三角函数以及欧拉公式即可。

当然现在激活函数的这种要求，也是基于整个计算是基于求导的。所以要求连续。利用signoid函数解决了连续的问题，而softmax则是解决概率分布的问题，同时正因为概率分布，函数要有界，这样概率和才能是一个定值。 但是网络就是一个函数链的模式，我们只是拿到了一个递归的模式，而得不到其通项公式，并且如果不是基于求导的，例如遗传算法呢，是不是就不要求激活函数的连续性了。
例外是如何利用分型混沌理论来解决这些结构。

http://blog.csdn.net/hyman_yx/article/details/51789186，没有激励函数的决定基，采用极数的思维，那就采用什么样基函数来拟合目标曲线，当然我们选择平滑的曲线要比直接要好。平滑的曲线那自然就得非线性了。

激活函数通常有如下一些性质：

非线性： 当激活函数是线性的时候，一个两层的神经网络就可以逼近基本上所有的函数了。但是，如果激活函数是恒等激活函数的时候（即f(x)=x），就不满足这个性质了，而且如果MLP使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的。
可微性： 当优化方法是基于梯度的时候，这个性质是必须的。
单调性： 当激活函数是单调的时候，单层网络能够保证是凸函数。
f(x)≈x： 当激活函数满足这个性质的时候，如果参数的初始化是random的很小的值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要很用心的去设置初始值。
输出值的范围： 当激活函数输出值是 有限 的时候，基于梯度的优化方法会更加 稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是 无限 的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的learning rate.
http://blog.csdn.net/cyh_24/article/details/50593400

现在明白各类函数的性质了。那决定了我们采用什么样的函数，激励函数决定了函数形状。

http://blog.csdn.net/cyh_24/article/details/50593400， 当然如果是ReLU的过，当learning rate很大的时候，那可能你的网络40%的神经元就dead了。
http://blog.csdn.net/cyh_24/article/details/50593400 这里列举了各种激活函数。


普适近似理论（universal approximation theorem）[1~2]表明，带有线性输出层和至少一层带有“挤压”性质的激活函数的隐含层的前馈神经网络，在隐含层具有足够神经元数的情况下，可以以任意精度近似任何一个从有限空间到另一个有限空间映射的Borel可测函数（定义在有界闭集上的任意连续函数是Borel可测的）。

所以，要想网络获得普适近似器的性质，一个必须点是“要有带有“挤压”性质的激活函数”。这里的“挤压”性质是因为早期对神经网络的研究用的是sigmoid类函数，所以对其数学性质的研究也主要基于这一类性质：将输入数值范围挤压到一定的输出数值范围。（后来发现，其他性质的激活函数也可以使得网络具有普适近似器的性质，如ReLU [3]）
这可以说是激活函数的理论作用了，使神经网络成为一个普适近似器

作者：Aewil Zheng
链接：https://www.zhihu.com/question/29021768/answer/109003954
来源：知乎
著作权归作者所有，转载请联系作者获得授权。


同时cost函数，以及activate函数固定了，网络的层数也差不多固定了，因为梯度vanishing与exploding会导致训练无效。W过大，在下一层导致a过小，因为行于函数两端。
因为不同的层对梯度也是不一样的。http://neuralnetworksanddeeplearning.com/chap5.html。 一般前面的层学习的更快。越靠后面，越敏感。
http://neuralnetworksanddeeplearning.com/chap5.html 公式121

梯度vanishing的原因，可以用wolfram求导，得到sigmoid函数一阶导，最大值是0.25. 再经过公式121. 十层 0.25^10 也就是9.5*10e-7. 相当于0.



激活函数是不是饱和，就看当前的activation值在哪里就知道了。所以每一层的activation值用图像表示出来。




神经元
======

y=ax+b 这个b,就相当于那个原点，只不过原点坐标，正好是0，0. 如何你需要把原点动一动位置的化，就需要添加一个bias. 所以如何确定bias,直接把系统输入都变成0，0时，你期望系统的输出是什么样子，然后根据这个确定bias.

同时y = ax+b 也是极数一种简写方式，b那就是0阶主分量了。
神经网络的映射能力与激活函数相关，因为它相当于是基函数。各种非线性函数都是可以当做激活函数的。激励函数的本质是数据密度指标函数，可以神经网络与聚类分析法有机结合起来。

其实核函数也可以指数函数，或者其他非线性函数。或者是一个状态机。

当然计算模式也不一定非得是一种固定的模式，例如也可以是径向基的模式，直接用输入与参数之间的距离值来当激活函数输入。这样是不是更有意义之些呢，因为这样直接具有距离的意义。

info flow
==========

auto coder， sparse coding。中层打label。

容量度量
=========

深度网络能够识别分类多少信息？也就是如何度量一个网络的识别能力。MC=N-(1-r^2N)


激活哪些神经元
***************


在神经网络中，有些神经元是没有激活的，怎样选择判定哪些神经元激活那？
是根据其W,B值来的，当然神经网络本身就是拟合非线性函数，其实就相当于极数的反算，求极限，如何找到其最终要表示的表达式。


最大化激活值
============

.. math::
   x^*= \arg \max_{x\, s.t.||x||=\pho} h_{ij}(\theta,x)
  
\theta 是权值，
通过这个函数，知道激活了哪些元素x，也算是一种可视化方法吧，另外这里是不是李刚说拓扑可视化吧。

但是和原来的完备化理论是否冲突？？

同时这个 softsign函数 x/(1+|x|) 也是一个激活函数。

从DBN的一个节点中采样
=====================

通过一个节点，使用DBN来确定对后续的影响，这个节点是相互独立的吗？

参考：
#. http://blog.csdn.net/zouxy09/article/details/10012747


training/validation/test
========================

测试数据不能做为training不然就会过拟合的问题。
而可以测试数据分几部分用，例如对参数的的求解。还有模型的优化。
http://www.cnblogs.com/bourneli/archive/2013/03/11/2954060.html

可视化
======

对于神经网络的可视化分析，持续的观察acivtations 与以及层与层之间的梯度变化。以及error随着时间的变化，都是很有意义的。
这些这些sigmoid函数应该可能避免初值很小或者很大的情况，那样会造成训练的无效。

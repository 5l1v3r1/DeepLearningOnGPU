基本问题
=========

#. 存储问题
#. 速度问题

现在常见的模型，像图像分类里面，微软设计的深度残差网络，模型大小差不多都在500M以上。自然语言处理的一些模型，例如语言模型（language
modeling）随着词表的增长而变大，可以有几G、几十G的大小，机器翻译的模型也都是500兆以上。当然500M的大小大家可能觉得没有多大，一个CPU服务器很容易就把这个模型给load进去使用。但是大家要注意到，很多时候深度学习的模型需要在一些移动设备上使用。比如说手机输入法，还有各种对图像做变换做处理做艺术效果的app，如果使用深度学习的话效果会非常好，但是这种模型由于它们的size太大，就不太适合在手机上应用。大家可以设想一下，如果一个手机的app需要加载一个500M甚至1G以上的模型恐怕不太容易被用户接受。

大计算说起来容易，其实做起来非常不容易，非常不简单。我们微软亚洲研究院研究员提出深度残差网络，这种网络如果在ImageNet这样一个上百万的数据上进行训练的话，用四块现在最先进的GPU卡K80学习训练时间大概要三周。最近百度做的神经机器翻译系统，他们用了32块K40的GPU用了十天做训练，谷歌的机器翻译系统用了更多，用了96块K80的GPU训练了六天。大家可能都知道AlphaGo，它也需要非常大量的计算资源。AlphaGo的模型包含一个策略神经网络，还有一个值网络，这两个都是卷积神经网络。它的策略网络用了50块GPU做训练，训练了3个周，值网络也是用了50块GPU，训练了一周，因此它整个的训练过程用了50块CPU四周时间，差不多一个月。大家可以想一想，如果训练一个模型就要等一个月，并且我们经常要调各种超参数，一组超参数得到的结果不好，换另外一组超参数，可能要尝试很多组超参数，如果我们没有大量的计算资源，一等就是一个月，这从产品的更新换代还有技术创新的角度而言，都不能接受。刚才说了只是AlphaGo训练的复杂度，其实它的测试，比如说比赛的时候，复杂度也非常高，
AlphaGo的单机版和人下棋的时候，每次下棋需要用48块CPU，8块GPU，它的分布式版本就用的更多，每次需要用1200块CPU再加上176块GPU。大家可以想一想，地球上有几个公司能承受这么高昂的代价来做深度学习。

并且这些大部分是针对CNN网络的优化。

这个就相当于传统代码优化了
==========================

#. 剪枝， 权重特别点直接扔了 迭代剪枝差不多能达到13X倍。 在进一步训练呢，提高了之后再优化。对于存储需要sparse matrix的存储，也就行压缩或者例压缩。 同样压缩的步强等等都是超参数。
   - 如何剪枝呢，如何显示不同神经元的重要性，就要通过可视化，常试删除对比操作，来确定哪些是重要的神经元，也些是不重要的。 例如DeepMind提出神经元删除法：通过理解每个神经元来理解深度学习 https://mp.weixin.qq.com/s/LTtcD5CBT8m127thjBaRsw
   - 把训练的神经网络，把每一个神经元的坐标化，然后通过标准图像来得到神经的激活坐标。然后来进一步优化。
   - 把网络拓扑 tree 化，就像DSSPN 就是一个很好，设计时优化的例子，同时结合传统的数据结构理论。 https://arxiv.org/pdf/1803.06067.pdf
#. 权值共享, 把这些参数直接进行聚类计算，然后用每一组的均值代替原来的参数。
#. 在共享的基础上，进一步量化，也就降低精度。极端情情况那就是二制神经网络.原来一个32bit权值现在只需要一个bit来表达。从而大大降低这个模型的尺寸。 
#. 最终的核心则计算出极值从而抽象符号。这样大大提高复用的价值。符号是质变，数值是量变。

#. 还有一个算法设计时的优化，因为神经网络本身的计算也都是冗余的，神经网络也是万能的函数拟合器，充分利用中间信息，然后添加合适的旁路层+合适的cost函数，直接输出需要的结果。而不是像R-CNN这样，只用一点点的DL，剩下还是传统方法计算，但这些完全可以用DL来替换掉。 如何要实现层次FPN金字塔，直接在CNN中取合适层，就可以了，不需要额外的构造，这也就是SSD使用的方法。 但是取哪些层，这就需进一步研究与试验了。

   - mobileNet 通过卷积的group相当于batch化，一个进行分解从而减少计算量。
#. 结构比较深度更重要。

#. Ablation study, 最简单的测试原则，看看有与没有有什么区别。

.. image:: /Stage_2/reduce/reduce_benchmark.jpg

自然语言相关的应用中，模型之所以大，是因为我们需要把每一个词要做词嵌入（word embedding），把每一个单词表达成向量空间的一个向量。词嵌入的基本思想是，语义相似或相近的词在向量空间里面的向量也比较接近，这样就可以通过向量空间表达词之间的语义信息或者是相似性。因为通常我们的词表会很大，比如说在输入法里面，可能词表需要说上百万。如果我们词表有上百万的词，每个词如果是用一千维的一个向量来表达，这个大小就是差不多是一百万乘以一千再乘以4 Byte（用32位的浮点数来表达），词嵌入向量的总体大小差不多就有4G左右，所以整个RNN模型是非常大的。搜索引擎的词表有上千万的词，仅仅词嵌入向量这部分大小就有40G左右，考虑到输入的词嵌入和输出的词嵌入，整个词嵌入的大小有80G左右了，这么大的模型很难加载到GPU上训练模型和使用，更不用说放在移动设备上使用。

https://www.msra.cn/zh-cn/news/features/tao-qin-machine-learning-20170309


DeepThin
========

DeepThinhook了Tensorflow的API，然后然后低秩分解。


DL 的优化这里才体现了其数学基础, 解决各种等价分解计算。想做一个training的时候就开发分解的方法。 
m*n的矩阵，如果秩很低就可以变成一个m*r,r*n的矩阵之积，但是后面两个矩阵所占用的存储空间比原来的m*n矩阵型小的得多。

传统的编解码的overhead太高了。 HashedNetworks, 相当于去除重复，相当于做clstering. 但是增加hash的计算量。

HyperNetwork
============

相当于内部用一个小网生成这个wights matrix.不过这个同样需要手工调试。


SqueezeNet->MobileNet->ShuffleNet->Xception
===========================================

SqueezeNet 就是充分利用了1*1 卷积来降维减少通道数。并且是把原来一层变成两层firemodule,1x1+{1x1*3,3}然后拼接起来。
https://blog.csdn.net/xbinworld/article/details/50897870

MobileNet 采用 depth-wise Separable convolution. 
   - Width Multiplier
   - Resolution Multitpler

ShuffleNet Shuffle for Group convolution https://github.com/gwli/ShuffleNet
Xception 基于Inception V3. 

ShuffleSeg在ShuffleSeg的基础上加上SKip Connections来提高分割的精度。

这些实现算法，都需要对framework本身的API进行扩展修改。 
轻型化网络

UNet 主要是用于 Feature fusion, 两种都可以，elementwise addition, feature concatenation, 但是前者计算效率更高。因此此文认为Transposed convolution 在UNet里的意义不大。


二值网络 
========


稀疏与低秩矩阵复原、模型压缩、二值网络. 这些精度的改变需要在framework这一层支持，例如tensorrt就支持。
关键是精度的下降会更很多，但是直接在到二值网络上训练呢。 
https://www.jiqizhixin.com/articles/2017-07-03-5
直接用位运算来代替加乘，是不是更合FPGA的结构。

大疆提供两个思路，用多个二值权值张量的线性组合近似浮点权值张量，用多个二值激活值张量来减少信息由于二值 化带来的损失。 

其实我们是不是可以直接提供现有计算直接转换位二值计算。并提供一个网络来提供这样的转换与训练适配工作。
https://github.com/hpi-xnor/BMXNet

或者直接二值层进行训练，相当直接利用与非门化简深度网络，进行卡诺图的电路设计了。

难点在于如何实现二制的微分与差分。与或非的微分计算。


还有把reduce直接放在训练里面，这是不是正是automl在干的事情。当然精度达到了之后，就开始进行尺寸的变化。

例如利用控制论的东东，例如利普 | f(b)-f(a) | < b -a 利普希茨常数。
https://zh.wikipedia.org/wiki/%E5%88%A9%E6%99%AE%E5%B8%8C%E8%8C%A8%E9%80%A3%E7%BA%8C
李雅普诺夫稳定性 网络，借控制理论来解决过拟合的问题

优化的方向
==========

也是网络扩扑从无序到有序的过程，如何找到那些无用计算，然后把它们直接删除掉呢。 
原来用bag of word->bag of feature-> bag of feature with weight-> bag of feature with weight and order. 这也就
是进化的方向

直接在训练时参数加入对于W稀疏约束。直接在训练的时候加入对网络W的优化约束

sparse coding 相当于一些抽象特征获取。
分类，找基，相同点，个数就是基的个数，例如所有的人都有五官，那五官做为基，来进行分类，然后就每一个基之间的矩离。
如何找相同点那就是计算SVD分解或者最大向量的最大的无关组，也就是秩。
也就是只是找到了维度，然后找到多种基表达式，例如正交坐标还是极坐标等等。
然后是计算坐标。
然后再设计可区分度，那就是WX+B 的那个B，那就是B就是分辨率， 空间的体积。

同时对于每一层输出的feature也要求稀疏优化。不光是对W稀疏，通过观察每一层输出，看看其是不是稀疏的，
然后再决定是不是扩大与缩小此层。

如何传统的知识 矩阵的分解以及极数的等价变换来实现化简。
例如 线性activiate+ 最小二乘 = sigmoid + cross-entropy. 

http://neuralnetworksanddeeplearning.com/chap3.html
如何化简这些东东，事先能够知道哪些一样的，同样可以网络更加简化。

如何实现拓扑的自动优化
======================

例如googlenet,V1,V2,V3等都是通过手工调试优化出来的，有没有办法自动优化呢。是不是相当于超参数的优化。

例如一次的W,B值从原来的成功的网络中来的。来找到这个边界。



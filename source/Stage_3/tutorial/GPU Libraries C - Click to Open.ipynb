{
 "metadata": {
  "name": "GPU Libraries C - Click to Open"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "# Accelerating C/C++ code with Libraries on GPUs\n\nIn this self-paced, hands-on lab, we will use libraries to accelerate code on NVIDIA GPUs.\n\nLab created by Mark Ebersole (Follow [@CUDAHamster](https://twitter.com/@cudahamster) on Twitter)"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "The following timer counts down to a five minute warning before the lab instance shuts down.  You should get a pop up at the five minute warning reminding you to save your work!"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<script src=\"files/countdown_v5.0/countdown.js\"></script>\n<div id=\"clock\" align=\"center\"></div>\n<script>\nmyDate = new Date();\ncurTime = Date.UTC(myDate.getUTCFullYear(), \n                   myDate.getUTCMonth(), \n                   myDate.getUTCDate(), \n                   myDate.getUTCHours(), \n                   myDate.getUTCMinutes(),\n                   myDate.getUTCSeconds(),\n                   myDate.getUTCMilliseconds());\n\nfunction countdownComplete(){\n  \talert(\"You only have five minutes left in the lab! Time to save your work - see the Post Lab section near the bottom.\");\n}\nvar myCD = new Countdown({\n                         time  \t: (1490831702406+110*60000-curTime)/1000,\n                         target\t \t: \"clock\",\n                         onComplete\t: countdownComplete,\n                         rangeHi  : \"minute\",\n                         hideLine\t: true,\n                         hideLabels\t: false,\n                         height\t \t: 60,\n                         width     : 150,\n                         style     : \"boring\",\n                    });\n </script>"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "---\nBefore we begin, let's verify [WebSockets](http://en.wikipedia.org/wiki/WebSocket) are working on your system.  To do this, execute the cell block below by giving it focus (clicking on it with your mouse), and hitting Ctrl-Enter, or pressing the play button in the toolbar above.  If all goes well, you should see get some output returned below the grey cell.  If not, please consult the [Self-paced Lab Troubleshooting FAQ](https://developer.nvidia.com/self-paced-labs-faq#Troubleshooting) to debug the issue."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "print \"The answer should be three: \" + str(1+2)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "Let's execute the cell below to display information about the GPUs running on the server."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "!nvidia-smi",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "---\n<p class=\"hint_trigger\">If you have never before taken an IPython Notebook based self-paced lab from NVIDIA, click this green box.\n      <div class=\"toggle_container\"><div class=\"input_area box-flex1\"><div class=\\\"highlight\\\">The following video will explain the infrastructure we are using for this self-paced lab, as well as give some tips on it's usage.  If you've never taken a lab on this system before, it's highly encourage you watch this short video first.<br><br>\n<div align=\"center\"><iframe width=\"640\" height=\"390\" src=\"http://www.youtube.com/embed/ZMrDaLSFqpY\" frameborder=\"0\" allowfullscreen></iframe></div>\n<br>\n<h2 style=\"text-align:center;color:red;\">Attention Firefox Users</h2><div style=\"text-align:center; margin: 0px 25px 0px 25px;\">There is a bug with Firefox related to setting focus in any text editors embedded in this lab. Even though the cursor may be blinking in the text editor, focus for the keyboard may not be there, and any keys you press may be applying to the previously selected cell.  To work around this issue, you'll need to first click in the margin of the browser window (where there are no cells) and then in the text editor.  Sorry for this inconvenience, we're working on getting this fixed.</div></div></div></div></p>"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "## Introduction to GPU Libraries\n\nGPU-accelerated libraries are a great and easy way to quickly speed-up the computationally intensive portions of your code.  You are able to access highly-tuned and GPU optimized algorithms, without having to write any of that code.\n\nThis lab consists of three tasks that will require you to modify some code, compile and execute it.  For each task, a solution is provided so you can check your work or take a peek if you get lost.\n\nIf you are still confused now, or at any point in this lab, you can consult the <a href=\"#FAQ\">FAQ</a> located at the bottom of this page."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "### Task #1\n\nFor this first task, we'll be using the [cuBLAS](https://developer.nvidia.com/cuBLAS) GPU-accelerated library to do a basic matrix multiply.  The specific API we'll be using is `cublasSgemm` (the `S` stands for `single` and the `gemm` for **GE**neric **M**atrix-**M**atrix Multiply) and you'll want to use the documentation for this call located at docs.nvidia.com [here](http://docs.nvidia.com/cuda/cublas/index.html#cublas-lt-t-gt-gemm).\n\nIt is important to realize that the GPU has its own physical memory, just like the CPU uses system RAM for its memory.  For the library we'll be using in this task, we have to ensure the data required is first copied across the PCI-Express bus to the GPU's memory before we call a library API.  For this task, we will manage the GPU memory with the following calls (detailed documentation [here](http://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__MEM.html#group__CUDA__MEM) at [docs.nvidia.com](http://docs.nvidia.com/)):\n\n* `cudaMalloc ( void** devPtr, size_t size )` - this API call is used to allocate memory on the GPU, and is very similar to using `malloc` on the CPU.  You provide the addres of a pointer that will point to the memory after the call completes successfully, as well as the number of bytes to allocate.\n* `cudaMemcpy ( void* dst, const void* src, size_t count, cudaMemcpyKind kind )` - also very similar the standard `memcpy`, this API call is used to copy data between the CPU and GPU.  It takes a destination pointer, a source pointer, the number of bytes to copy, and the fourth parameter indicates which direction the data is traveling: GPU->CPU, CPU->GPU, or GPU->GPU.  The two constants we'll use for Task #2 are:\n * cudaMemcpyDeviceToHost - data is traveling from the GPU to the CPU\n * cudaMemcpyHostToDevice - data is traveling from the CPU to the GPU \n* `cudaFree ( void* devPtr )` - we use this API call to free any memory we allocated on the GPU.\n\nIn the code below, there already exists a single-threaded CPU version of matrix multiply we'll use to compare our GPU library call answer with.  Your goal is to replace the `## FIXME: ... ##` regions of code to successfully call `cublasSgemm` and do a basic matrix multiply.  If you get stuck, there are a number of hints provided below.  In addition, I encourage you to look at the task1_solution.cu file (click on the task1 folder in the editor below to see the file) and compare your work, or use this if you are lost.\n\nWhen you are ready to compile and run, there are two executable cells below the text editor.  The first one compiles task1.cu, and the second runs the resulting program.  Remember you execute these cells with Ctrl-Enter, or by pressing the Play button in the toolbar at the top of the window.\n\n<p class=\"hint_trigger\">Hint #1\n      <div class=\"toggle_container\"><div class=\"input_area box-flex1\"><div class=\\\"highlight\\\">The <code>cublasHandle_t</code> required by the <code>cublasSgemm</code> call has already been created for you in the code.</div></div></div></p>\n\n<p class=\"hint_trigger\">Hint #2\n      <div class=\"toggle_container\"><div class=\"input_area box-flex1\"><div class=\\\"highlight\\\">You will need to use <code>cudaMalloc</code> to  allocate space for the <code>d_a</code>, <code>d_b</code>, and <code>d_c</code> arrays on the GPU.  These pointers have already been declared for you.</div></div></div></p>\n      \n<p class=\"hint_trigger\">Hint #3\n      <div class=\"toggle_container\"><div class=\"input_area box-flex1\"><div class=\\\"highlight\\\">Once you have allocated the arrays on the GPU, you will need to copy the <code>a</code> and <code>b</code> arrays to the GPU using <code>cudaMemcpy</code> calls.  After the <code>cublasSgemm</code> call has returned, you'll want to use <code>cudaMemcpy</code> to copy the resulting <code>d_c</code> array back to the host.</div></div></div></p>\n      \n<p class=\"hint_trigger\">Hint #4\n      <div class=\"toggle_container\"><div class=\"input_area box-flex1\"><div class=\\\"highlight\\\">You'll need to use the cublasSgemm <a href=\"http://docs.nvidia.com/cuda/cublas/index.html#cublas-lt-t-gt-gemm\">documentation</a> to determine the remaining parameters needed in the call.</div></div></div></p>"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<iframe id=\"task1\" src=\"task1\" width=\"100%\" height=\"500px\">\n  <p>Your browser does not support iframes.</p>\n</iframe>"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Execute this cell to compile ttask1\n!nvcc -arch=sm_20 -lcublas -o task1_out task1/task1.cu && echo Compiled Successfully!",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Compiled Successfully!\r\n"
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "!./task1_out",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Matrix size is 1024\r\n"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Launching CPU sgemm\r\n"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Total time CPU is 9.425815 sec\r\nPerformance is 0.227830 GFlop/s\r\n"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Launching GPU sgemm\r\nTotal time GPU CUBLAS is 0.006599 sec\r\nPerformance is 325.425617 GFlop/s\r\nerror is 0.000194\r\n"
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "With just a few lines of code, we were able to call a highly optimized version of matrix-multiply, instead of having to write that code ourselves.  And with each new generation of GPUs, the libraries will be updated to continually take advantage of enhanced performance and features.\n\nIn the above task, we only used a single library call.  However, the real power of libraries tends to come from stringing multiple calls together to build a more in-depth algorithm.  We'll explore using multiple GPU libraries in the next task."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "### Task #2\n\nFor this task, we're going to make use of the GPU to create the random numbers used in our matrix multiplication.  While this is a contrived example, it's a good simplistic way to show the concepts of using multiple GPU libraries in tandem.\n\nThe CPU-side matrix multiply has been removed, and we'll just being using the cublasSgemm to do the multiply.\n\nYour objective in this task is replace the for-loop and rand() calls with two host-side version of the [cuRAND](https://developer.nvidia.com/cuRAND) `curandGenerateNormal` API call.  You'll want to look at the cuRAND [documentation](http://docs.nvidia.com/cuda/curand/host-api-overview.html#generation-functions) for this to see what you need to do.  \n\n**Before** you modify any code, you should compile and execute task2.cu using the two cells below the text editor.  This will give you a benchmark on how long the original code takes to run."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<p class=\"hint_trigger\">Hint #1\n      <div class=\"toggle_container\"><div class=\"input_area box-flex1\"><div class=\\\"highlight\\\">For this task, make sure to use the Host version of the <code>curandCreateGeneratorHost</code> call.  Well explore the device-side version in the next task.</div></div></div></p>"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<p class=\"hint_trigger\">Hint #2\n      <div class=\"toggle_container\"><div class=\"input_area box-flex1\"><div class=\\\"highlight\\\">There's a great example showing how to use the cuRAND library located <a href=\"http://docs.nvidia.com/cuda/curand/host-api-overview.html#host-api-example\">here</a>.</div></div></div></p>"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<p class=\"hint_trigger\">Hint #3\n      <div class=\"toggle_container\"><div class=\"input_area box-flex1\"><div class=\\\"highlight\\\">There is no need to add any <code>cudaMempcy</code> calls in this task.</div></div></div></p>"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<iframe id=\"task2\" src=\"task2\" width=\"100%\" height=\"500px\">\n  <p>Your browser does not support iframes.</p>\n</iframe>"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Execute this cell to compile task2.cu\n!nvcc -arch=sm_20 -lcublas -lcurand -o task2_out task2/task2.cu && echo Compiled Successfully!",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Compiled Successfully!\r\n"
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "!./task2_out",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Matrix size is 10000\r\n"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Create random numbers\r\n"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Launching GPU sgemm\r\n"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Total time is 2.074910 sec\r\n"
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "If you have successfully used the cuRAND library to create random data for matrices `h_a` and `h_b`, and used the `curandCreateGeneratorHost` call, you will probably have noticed that the whole program takes **longer** to run than the original.  That's definitely not what we want or expect when moving computation to the GPU.  So what gives?\n\nThis is a great opportunity to demonstrate the command-line profile provided in the [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit) from NVIDIA; nvprof.  Execute the below cell to profile your task2_out program and we'll see if we can figure out what the problem is.  This is called profiler-driven optimization."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "!nvprof ./task2_out",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Matrix size is 10000\r\n"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "==17412== NVPROF is profiling process 17412, command: ./task2_out\r\n"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Create random numbers\r\n"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Launching GPU sgemm\r\n"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Total time is 2.095588 sec\r\n==17412== Profiling application: ./task2_out\r\n==17412== Profiling result:\r\nTime(%)      Time     Calls       Avg       Min       Max  Name\r\n 80.03%  1.66369s         1  1.66369s  1.66369s  1.66369s  sgemm_sm30_ldg_tex_nn_64x16x64x16x16\r\n 11.03%  229.24ms         3  76.415ms  1.1840us  115.45ms  [CUDA memcpy HtoD]\r\n  8.23%  171.13ms         1  171.13ms  171.13ms  171.13ms  [CUDA memcpy DtoH]\r\n  0.71%  14.659ms         1  14.659ms  14.659ms  14.659ms  generate_seed_pseudo(__int64, __int64, __int64, curandOrdering, curandStateXORWOW*, unsigned int*)\r\n  0.00%  19.967us         2  9.9830us  9.3430us  10.624us  void gen_sequenced<curandStateXORWOW, float, int, __operator_&__(float curand_uniform_noargs<curandStateXORWOW>(curandStateXORWOW*, int))>(curandStateXORWOW*, float*, unsigned long, unsigned long, int)\r\n\r\n==17412== API calls:\r\nTime(%)      Time     Calls       Avg       Min       Max  Name\r\n 88.68%  2.06548s         4  516.37ms  21.970us  1.83555s  cudaMemcpy\r\n 11.17%  260.20ms         9  28.911ms  39.072us  231.51ms  cudaFree\r\n  0.11%  2.5630ms         8  320.38us  9.6940us  817.90us  cudaMalloc\r\n  0.02%  356.00us       249  1.4290us     204ns  50.193us  cuDeviceGetAttribute\r\n  0.01%  195.27us         2  97.634us  93.713us  101.56us  cudaGetDeviceProperties\r\n  0.00%  83.723us         4  20.930us  9.0130us  37.903us  cudaLaunch\r\n  0.00%  54.243us         3  18.081us  15.904us  22.036us  cuDeviceGetName\r\n  0.00%  32.617us         3  10.872us  10.371us  11.621us  cuDeviceTotalMem\r\n  0.00%  28.511us         2  14.255us  3.3300us  25.181us  cudaCreateTextureObject\r\n  0.00%  14.582us        34     428ns     198ns  4.4810us  cudaSetupArgument\r\n  0.00%  13.672us         3  4.5570us  3.2480us  5.3560us  cudaThreadSynchronize\r\n  0.00%  10.028us         8  1.2530us     601ns  3.9760us  cudaEventCreateWithFlags\r\n  0.00%  9.0940us         8  1.1360us     692ns  2.8920us  cudaEventDestroy\r\n  0.00%  7.4700us        10     747ns     371ns  3.2640us  cudaDeviceGetAttribute\r\n  0.00%  7.3340us         3  2.4440us  1.0490us  3.5580us  cudaGetDevice\r\n  0.00%  5.4550us         2  2.7270us  1.1180us  4.3370us  cudaDestroyTextureObject\r\n  0.00%  4.2620us         4  1.0650us     343ns  2.2140us  cudaConfigureCall\r\n  0.00%  4.0060us         7     572ns     211ns  2.2010us  cudaGetLastError\r\n  0.00%  2.2960us         4     574ns     319ns  1.1190us  cuDeviceGetCount\r\n  0.00%  1.7330us         2     866ns     590ns  1.1430us  cuInit\r\n  0.00%  1.4520us         4     363ns     319ns     401ns  cuDeviceGet\r\n  0.00%  1.0750us         2     537ns     465ns     610ns  cuDriverGetVersion\r\n  0.00%     569ns         1     569ns     569ns     569ns  cudaCreateChannelDesc\r\n"
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "If your modified task2.cu file is similar to the task2_solution.cu code, the nvprof will show that the 2nd and 3rd areas consuming most of our application time are transfers between the Device and Host memory across the PCI-Express bus:\n\n    6.48%  498.69ms         1  498.69ms  498.69ms  498.69ms  [CUDA memcpy DtoH]\n    5.94%  456.66ms         3  152.22ms  1.2160us  228.77ms  [CUDA memcpy HtoD]\n    \nSo, our next task is to see if we can minimize some of these transfers.  The reason for these excessive data transfers lies in the fact that we used the host-side version of `curandCreateGeneratorHost`.  Random number creation using this generator will create random numbers on the GPU, and then copy the data across the PCI-Express bus to host-side arrays.  So what's currently happening is:\n\n1. Tell the GPU to create random numbers\n2. The cuRAND library then copies these random numbers across the PCI-Express bus to host memory\n3. We then re-copy these random numbers back to the GPU to be used by our `cublasSgemm` call.\n\nSo we're doing four extra transfers (2x arrays 2 times) of data than is required!"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "### Task #3\n\nYour final task will be to modify task3.cu (the same as task2_solution.cu in Task #2) to minimize the number of transfers across the PCI-Express bus.  Please make use of the cuRAND documentation provided above, if needed."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<p class=\"hint_trigger\">Hint #1\n      <div class=\"toggle_container\"><div class=\"input_area box-flex1\"><div class=\\\"highlight\\\">To have the <code>cuRAND</code> library fill a device-side array, you'll want to use <code>curandCreateGenerator</code> without the <code>Host</code> on the end.</div></div></div></p>"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<p class=\"hint_trigger\">Hint #2\n      <div class=\"toggle_container\"><div class=\"input_area box-flex1\"><div class=\\\"highlight\\\">You'll no longer need the <code>cudaMemcpy</code> to move the <code>h_a</code> and <code>h_b</code> arrays to the device since the <code>curandGenerateNormalDouble</code> will have already filled in the device arrays!</div></div></div></p>"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<iframe id=\"task3\" src=\"task3\" width=\"100%\" height=\"500px\">\n  <p>Your browser does not support iframes.</p>\n</iframe>"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Execute this cell to compile task3.cu\n!nvcc -arch=sm_20 -lcublas -lcurand -o task3_out task3/task3.cu && echo Compiled Successfully!",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Compiled Successfully!\r\n"
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "!./task3_out",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Matrix size is 10000\r\n"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Create random numbers\r\n"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Launching GPU dgemm\r\n"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Total time is 13.352980 sec\r\n"
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "In my case, with the modification to the random number generation, our contrived example is now running about three times as fast as we started with - and we did not have to write any GPU specific code, just make a few library API calls.\n\nYou can now see how leaving data on the GPU for as long as possible, and stringing together API calls from various libraries, can provide great flexibility in creating extremely fast and powerful algorithms!"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "## Learn More\n\nIf you are interested in learning more, you can use the following resources:\n\n* Learn more at the [CUDA Developer Zone](https://developer.nvidia.com/category/zone/cuda-zone).\n* See a list of available libraries [here](https://developer.nvidia.com/gpu-accelerated-libraries).\n* If you have an NVIDIA GPU in your system, you can download and install the [CUDA tookit](https://developer.nvidia.com/cuda-toolkit) to access NVIDIA's GPU-accelerated libraries.\n* Search or ask questions on [Stackoverflow](http://stackoverflow.com/questions/tagged/cuda) using the cuda tag."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<a id=\"post-lab\"></a>\n## Post-Lab\n\nFinally, don't forget to save your work from this lab before time runs out and the instance shuts down!!\n\n1. Save this IPython Notebook by going to `File -> Download as -> IPython (.ipynb)` at the top of this window\n2. You can execute the following cell block to create a zip-file of the files you've been working on, and download it with the link below."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "%%bash\nrm -f library_c_files.zip\nzip -r library_c_files.zip task1/*.cu task1/*.h task2/*.cu task2/*.h task3/*.cu task3/*.h",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "**After** executing the above cell, you should be able to download the zip file [here](files/library_c_files.zip)"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<a id=\"FAQ\"></a>\n---\n# Lab FAQ\n\nQ: I'm encountering issues executing the cells, or other technical problems?<br>\nA: Please see [this](https://developer.nvidia.com/self-paced-labs-faq#Troubleshooting) infrastructure FAQ."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "<style>\np.hint_trigger{\n  margin-bottom:7px;\n  margin-top:-5px;\n  background:#64E84D;\n}\n.toggle_container{\n  margin-bottom:0px;\n}\n.toggle_container p{\n  margin:2px;\n}\n.toggle_container{\n  background:#f0f0f0;\n  clear: both;\n  font-size:100%;\n}\n</style>\n<script>\n$(\"p.hint_trigger\").click(function(){\n   $(this).toggleClass(\"active\").next().slideToggle(\"normal\");\n});\n   \n$(\".toggle_container\").hide();\n</script>"
    }
   ],
   "metadata": {}
  }
 ]
}